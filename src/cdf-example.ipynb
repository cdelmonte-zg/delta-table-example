{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_delta_operations.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile test_delta_operations.py\n",
    "\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, StructField, StructType, TimestampType\n",
    "from delta.tables import DeltaTable\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number, col, sum as _sum\n",
    "\n",
    "# Define schema with TimestampType\n",
    "schema = StructType([\n",
    "    StructField(\"sale_id\", IntegerType(), True),\n",
    "    StructField(\"product_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"amount\", IntegerType(), True),\n",
    "    StructField(\"sale_date\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for Gold table\n",
    "gold_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"total_amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    \"\"\"Fixture to provide a Spark session with Delta Lake support enabled.\"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PySpark Delta Testing\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .config(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\" ) \\\n",
    "        .getOrCreate()\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def setup_delta_tables(spark_session):\n",
    "    \"\"\"Setup and teardown of Delta tables used in the tests.\"\"\"\n",
    "    spark_session.sql(\"DROP TABLE IF EXISTS sales_bronze\")\n",
    "    spark_session.sql(\"DROP TABLE IF EXISTS sales_silver\")\n",
    "    spark_session.sql(\"DROP TABLE IF EXISTS sales_gold\")\n",
    "\n",
    "    data = [(1, 101, 1001, 200, datetime.strptime('2020-08-20 10:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "            (2, 102, 1002, 150, datetime.strptime('2020-08-20 10:00:00', '%Y-%m-%d %H:%M:%S'))]\n",
    "\n",
    "    # Setup Bronze table with date objects\n",
    "    bronze_path = \"/tmp/delta/sales_bronze\"\n",
    "    df = spark_session.createDataFrame(data, schema)\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(bronze_path)\n",
    "    spark_session.sql(f\"CREATE TABLE sales_bronze USING DELTA LOCATION '{bronze_path}'\")\n",
    "    spark_session.sql(\"ALTER TABLE sales_bronze SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true', 'delta.logRetentionDuration' = 'interval 0 seconds', 'delta.deletedFileRetentionDuration' = 'interval 0 seconds')\")\n",
    "    spark_session.sql(\"VACUUM sales_bronze RETAIN 0 HOURS\")\n",
    "\n",
    "    # Setup Silver table with date objects\n",
    "    silver_path = \"/tmp/delta/sales_silver\"\n",
    "    df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(silver_path)\n",
    "    spark_session.sql(f\"CREATE TABLE sales_silver USING DELTA LOCATION '{silver_path}'\")\n",
    "    spark_session.sql(\"ALTER TABLE sales_silver SET TBLPROPERTIES ('delta.enableChangeDataFeed' = 'true', 'delta.logRetentionDuration' = 'interval 0 seconds', 'delta.deletedFileRetentionDuration' = 'interval 0 seconds')\")\n",
    "    spark_session.sql(\"VACUUM sales_silver RETAIN 0 HOURS\")\n",
    "        \n",
    "    # Define and set up the Gold table\n",
    "    gold_path = \"/tmp/delta/sales_gold\"\n",
    "    df_empty_gold = spark_session.createDataFrame([], gold_schema)\n",
    "    df_empty_gold.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(gold_path)\n",
    "    spark_session.sql(f\"CREATE TABLE sales_gold USING DELTA LOCATION '{gold_path}'\")\n",
    "    spark_session.sql(\"VACUUM sales_gold RETAIN 0 HOURS\")\n",
    "\n",
    "    yield bronze_path, silver_path, gold_path\n",
    "\n",
    "    # Cleanup after tests\n",
    "    spark_session.sql(\"DROP TABLE IF EXISTS sales_bronze\")\n",
    "    spark_session.sql(\"DROP TABLE IF EXISTS sales_silver\")\n",
    "    spark_session.sql(\"DROP TABLE IF EXISTS sales_gold\")\n",
    "\n",
    "\n",
    "def simulate_data_flow_to_silver(spark_session, bronze_path, silver_path, starting_from_timestamp):\n",
    "    \"\"\"Simulate data updates and new data ingestion using CDF, taking only the last change.\"\"\"\n",
    "    updates = [(1, 101, 1001, 250, datetime.strptime('2020-08-21 10:00:00', '%Y-%m-%d %H:%M:%S')),  # Updated amount\n",
    "               (1, 101, 1001, 260, datetime.strptime('2020-08-21 11:00:00', '%Y-%m-%d %H:%M:%S')),  # Another update\n",
    "               (3, 103, 1001, 300, datetime.strptime('2020-08-22 10:00:00', '%Y-%m-%d %H:%M:%S'))]  # New sale\n",
    "    df_updates = spark_session.createDataFrame(updates, schema)\n",
    "    df_updates.write.format(\"delta\").mode(\"append\").save(bronze_path)\n",
    "\n",
    "    df_changes = spark_session.read.format(\"delta\") \\\n",
    "                                   .option(\"readChangeData\", \"true\") \\\n",
    "                                   .option(\"startingTimestamp\", starting_from_timestamp) \\\n",
    "                                   .table(\"sales_bronze\")\n",
    "\n",
    "    window_spec = Window.partitionBy(\"sale_id\").orderBy(col(\"sale_date\").desc())\n",
    "    df_latest_changes = df_changes.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "                                  .filter(\"rn = 1\") \\\n",
    "                                  .drop(\"rn\")\n",
    "\n",
    "    silver_table = DeltaTable.forPath(spark_session, silver_path)\n",
    "    (silver_table.alias(\"silver\")\n",
    "     .merge(\n",
    "         df_latest_changes.alias(\"updates\"),\n",
    "         \"silver.sale_id = updates.sale_id\")\n",
    "     .whenMatchedUpdateAll()\n",
    "     .whenNotMatchedInsertAll()\n",
    "     .execute())\n",
    "\n",
    "def simulate_data_flow_to_gold(spark_session, silver_path, gold_path, starting_from_timestamp):\n",
    "    \"\"\"Simulate data updates from Silver to Gold table using CDF.\"\"\"\n",
    "    df_silver_changes = spark_session.read.format(\"delta\").option(\"readChangeData\", \"true\").option(\"startingTimestamp\", starting_from_timestamp).table(\"sales_silver\")\n",
    "\n",
    "    window_spec = Window.partitionBy(\"sale_id\").orderBy(col(\"sale_date\").desc())\n",
    "    df_silver_latest = df_silver_changes.withColumn(\"rn\", row_number().over(window_spec)).filter(\"rn = 1\").drop(\"rn\")\n",
    "\n",
    "    df_silver_latest.createOrReplaceTempView(\"temp_latest_changes\")\n",
    "\n",
    "    \n",
    "    df_gold_aggregate = spark_session.sql(\"\"\"\n",
    "        SELECT customer_id, SUM(amount) AS total_amount\n",
    "        FROM temp_latest_changes\n",
    "        GROUP BY customer_id\n",
    "    \"\"\")\n",
    "\n",
    "    gold_table = DeltaTable.forPath(spark_session, gold_path)\n",
    "    gold_table.alias(\"gold\").merge(df_gold_aggregate.alias(\"updates\"), \"gold.customer_id = updates.customer_id\").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "def test_read_data(setup_delta_tables, spark_session):\n",
    "    \"\"\"Test to ensure data can be read from the Delta table.\"\"\"\n",
    "    bronze_path, _, _ = setup_delta_tables\n",
    "    df = spark_session.read.format(\"delta\").load(bronze_path)\n",
    "    expected_data = [(1, 101, 1001, 200, datetime.strptime('2020-08-20 10:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "                     (2, 102, 1002, 150, datetime.strptime('2020-08-20 10:00:00', '%Y-%m-%d %H:%M:%S'))]\n",
    "    expected_df = spark_session.createDataFrame(expected_data, schema)\n",
    "    \n",
    "    assert df.collect() == expected_df.collect(), \"Data read from Delta table does not match expected data\"\n",
    "\n",
    "def test_initial_sync(setup_delta_tables, spark_session):\n",
    "    bronze_path, silver_path, _ = setup_delta_tables\n",
    "    df_bronze = spark_session.read.format(\"delta\").load(bronze_path)\n",
    "    df_silver = spark_session.read.format(\"delta\").load(silver_path)\n",
    "    \n",
    "    assert df_bronze.collect() == df_silver.collect(), \"Initial data in Silver table does not match Bronze table\"\n",
    "\n",
    "\n",
    "def test_data_propagation_to_silver(setup_delta_tables, spark_session):\n",
    "    bronze_path, silver_path, _ = setup_delta_tables\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    simulate_data_flow_to_silver(spark_session, bronze_path, silver_path, timestamp)\n",
    "    df_silver = spark_session.read.format(\"delta\").load(silver_path)\n",
    "    expected_data = [(1, 101, 1001, 260, datetime.strptime('2020-08-21 11:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "                     (2, 102, 1002, 150, datetime.strptime('2020-08-20 10:00:00', '%Y-%m-%d %H:%M:%S')),\n",
    "                     (3, 103, 1001, 300, datetime.strptime('2020-08-22 10:00:00', '%Y-%m-%d %H:%M:%S'))]\n",
    "    expected_df = spark_session.createDataFrame(expected_data, schema)\n",
    "    \n",
    "    differences = df_silver.subtract(expected_df)\n",
    "    assert differences.count() == 0, \"Data in Silver table does not match expected data.\"\n",
    "\n",
    "def test_data_propagation_to_gold(setup_delta_tables, spark_session):\n",
    "    bronze_path, silver_path, gold_path = setup_delta_tables\n",
    "    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    simulate_data_flow_to_silver(spark_session, bronze_path, silver_path, timestamp)  # Ensure Silver table is updated first\n",
    "    simulate_data_flow_to_gold(spark_session, silver_path, gold_path, timestamp)\n",
    "    df_gold = spark_session.read.format(\"delta\").load(gold_path)\n",
    "    expected_data = [(1001, 560), (1002, 150)]\n",
    "    expected_df = spark_session.createDataFrame(expected_data, [\"customer_id\", \"total_amount\"])\n",
    "    df_gold.show()\n",
    "    differences = df_gold.subtract(expected_df)\n",
    "    assert differences.count() == 0, \"Data in Gold table does not match expected aggregated data.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.11.6, pytest-8.2.1, pluggy-1.5.0\n",
      "rootdir: /home/jovyan/work\n",
      "plugins: anyio-4.0.0\n",
      "collected 4 items\n",
      "\n",
      "test_delta_operations.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m                                            [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 25.83s\u001b[0m\u001b[32m ==============================\u001b[0m\n",
      "\n",
      "Errors: \n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Running pytest and capturing output\n",
    "result = subprocess.run(['pytest', 'test_delta_operations.py'], capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(\"Errors:\", result.stderr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
